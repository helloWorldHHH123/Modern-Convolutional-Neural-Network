# -*- coding:utf-8 -*-
'''
ä½œè€…ï¼šcy
æ—¥æœŸï¼š2025å¹´11æœˆ05æ—¥
7.5 æ‰¹é‡è§„èŒƒåŒ–

ä¸ºä»€ä¹ˆéœ€è¦æ‰¹é‡è§„èŒƒåŒ–å±‚?
é¦–å…ˆï¼Œæ•°æ®é¢„å¤„ç†çš„æ–¹å¼é€šå¸¸ä¼šå¯¹æœ€ç»ˆç»“æœäº§ç”Ÿå·¨å¤§å½±å“ã€‚
ç¬¬äºŒï¼Œå¯¹äºå…¸å‹çš„å¤šå±‚æ„ŸçŸ¥æœºæˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚å½“æˆ‘ä»¬è®­ç»ƒæ—¶ï¼Œä¸­é—´å±‚ä¸­çš„å˜é‡ï¼ˆä¾‹å¦‚ï¼Œå¤šå±‚æ„ŸçŸ¥æœºä¸­çš„ä»¿
å°„å˜æ¢è¾“å‡ºï¼‰å¯èƒ½å…·æœ‰æ›´å¹¿çš„å˜åŒ–èŒƒå›´ï¼šä¸è®ºæ˜¯æ²¿ç€ä»è¾“å…¥åˆ°è¾“å‡ºçš„å±‚ï¼Œè·¨åŒä¸€å±‚ä¸­çš„å•å…ƒï¼Œæˆ–æ˜¯éšç€æ—¶é—´
çš„æ¨ç§»ï¼Œæ¨¡å‹å‚æ•°çš„éšç€è®­ç»ƒæ›´æ–°å˜å¹»è«æµ‹ã€‚æ‰¹é‡è§„èŒƒåŒ–çš„å‘æ˜è€…éæ­£å¼åœ°å‡è®¾ï¼Œè¿™äº›å˜é‡åˆ†å¸ƒä¸­çš„è¿™ç§å
ç§»å¯èƒ½ä¼šé˜»ç¢ç½‘ç»œçš„æ”¶æ•›ã€‚
ç¬¬ä¸‰ï¼Œæ›´æ·±å±‚çš„ç½‘ç»œå¾ˆå¤æ‚ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚è¿™æ„å‘³ç€æ­£åˆ™åŒ–å˜å¾—æ›´åŠ é‡è¦ã€‚

æ‰¹é‡è§„èŒƒåŒ–åŸç†ï¼š
æ‰¹é‡è§„èŒƒåŒ–åº”ç”¨äºå•ä¸ªå¯é€‰å±‚ï¼ˆä¹Ÿå¯ä»¥åº”ç”¨åˆ°æ‰€æœ‰å±‚ï¼‰ï¼Œå…¶åŸç†å¦‚ä¸‹ï¼šåœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè§„èŒƒåŒ–è¾“
å…¥ï¼Œå³é€šè¿‡å‡å»å…¶å‡å€¼å¹¶é™¤ä»¥å…¶æ ‡å‡†å·®ï¼Œå…¶ä¸­ä¸¤è€…å‡åŸºäºå½“å‰å°æ‰¹é‡å¤„ç†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”ç”¨æ¯”ä¾‹ç³»æ•°å’Œæ¯”
ä¾‹åç§»ã€‚æ­£æ˜¯ç”±äºè¿™ä¸ªåŸºäºæ‰¹é‡ç»Ÿè®¡çš„æ ‡å‡†åŒ–ï¼Œæ‰æœ‰äº†æ‰¹é‡è§„èŒƒåŒ–çš„åç§°ã€‚

æ‰¹é‡è§„èŒƒåŒ–å±‚åœ¨â€è®­ç»ƒæ¨¡å¼â€œï¼ˆé€šè¿‡å°æ‰¹é‡ç»Ÿè®¡æ•°æ®è§„èŒƒåŒ–ï¼‰å’Œâ€œé¢„æµ‹æ¨¡å¼â€ï¼ˆé€šè¿‡æ•°æ®é›†ç»Ÿè®¡è§„èŒƒåŒ–ï¼‰
ä¸­çš„åŠŸèƒ½ä¸åŒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ— æ³•å¾—çŸ¥ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ¥ä¼°è®¡å¹³å‡å€¼å’Œæ–¹å·®ï¼Œæ‰€ä»¥åªèƒ½æ ¹æ®æ¯ä¸ªå°æ‰¹
æ¬¡çš„å¹³å‡å€¼å’Œæ–¹å·®ä¸æ–­è®­ç»ƒæ¨¡å‹ã€‚è€Œåœ¨é¢„æµ‹æ¨¡å¼ä¸‹ï¼Œå¯ä»¥æ ¹æ®æ•´ä¸ªæ•°æ®é›†ç²¾ç¡®è®¡ç®—æ‰¹é‡è§„èŒƒåŒ–æ‰€éœ€çš„å¹³å‡å€¼
å’Œæ–¹å·®ã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬å°†æ‰¹é‡è§„èŒƒåŒ–å±‚ç½®äºå…¨è¿æ¥å±‚ä¸­çš„ä»¿å°„å˜æ¢å’Œæ¿€æ´»å‡½æ•°ä¹‹é—´ã€‚
å¯¹äºå·ç§¯å±‚ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å·ç§¯å±‚ä¹‹åå’Œéçº¿æ€§æ¿€æ´»å‡½æ•°ä¹‹å‰åº”ç”¨æ‰¹é‡è§„èŒƒåŒ–ã€‚å½“å·ç§¯æœ‰å¤šä¸ªè¾“å‡ºé€šé“
æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™äº›é€šé“çš„â€œæ¯ä¸ªâ€è¾“å‡ºæ‰§è¡Œæ‰¹é‡è§„èŒƒåŒ–ï¼Œæ¯ä¸ªé€šé“éƒ½æœ‰è‡ªå·±çš„æ‹‰ä¼¸ï¼ˆscaleï¼‰å’Œåç§»ï¼ˆshiftï¼‰
å‚æ•°ï¼Œè¿™ä¸¤ä¸ªå‚æ•°éƒ½æ˜¯æ ‡é‡ã€‚
'''

"""
å‡ ä¸ªæ˜“æ··ç‚¹ï¼š
æ­£åˆ™åŒ–æŠ€æœ¯ï¼šDropout 
ä»¿å°„å˜æ¢ï¼šçº¿æ€§å˜æ¢ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ + å¹³ç§»ï¼ˆåç½®åŠ æ³•ï¼‰ã€‚å…¨è¿æ¥å±‚æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªä»¿å°„å˜æ¢ï¼šoutput = input Ã— weight + biasã€‚
æ¿€æ´»å‡½æ•°ï¼šTanhï¼ˆè¾“å‡ºèŒƒå›´(-1,1)ï¼Œé›¶ä¸­å¿ƒï¼‰      
        Sigmoidï¼ˆè¾“å‡ºèŒƒå›´(0,1)ï¼Œæ˜“äºè§£é‡Šä¸ºæ¦‚ç‡ï¼‰
        Softmaxï¼ˆå°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ€»å’Œä¸º1ï¼‰
        ReLUï¼ˆè®¡ç®—ç®€å•ï¼Œæ”¶æ•›å¿«ï¼›ç¼“è§£æ¢¯åº¦æ¶ˆå¤±	ï¼‰
        Leaky ReLUã€PReLUã€ELUã€GELUã€Swish
"""

import Section01
import torch
from torch import nn

# 7.5.3 ä»é›¶å®ç°ã€‚ä»å¤´å¼€å§‹å®ç°ä¸€ä¸ªå…·æœ‰å¼ é‡çš„æ‰¹é‡è§„èŒƒåŒ–å±‚ã€‚
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # é€šè¿‡is_grad_enabledæ¥åˆ¤æ–­å½“å‰æ¨¡å¼æ˜¯è®­ç»ƒæ¨¡å¼è¿˜æ˜¯é¢„æµ‹æ¨¡å¼
    if not torch.is_grad_enabled():
        # å¦‚æœæ˜¯åœ¨é¢„æµ‹æ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç§»åŠ¨å¹³å‡æ‰€å¾—çš„å‡å€¼å’Œæ–¹å·®
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        # æ–­è¨€å¼ é‡ X çš„ç»´åº¦æ˜¯2ç»´æˆ–4ç»´
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # ä½¿ç”¨å…¨è¿æ¥å±‚çš„æƒ…å†µï¼Œè®¡ç®—ç‰¹å¾ç»´ä¸Šçš„å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # å‡è®¾ X æ˜¯ä¸€ä¸ª4Då¼ é‡ï¼Œå½¢çŠ¶ä¸ºï¼š[batch_size, channels, height, width]
            # ä½¿ç”¨äºŒç»´å·ç§¯å±‚çš„æƒ…å†µï¼Œè®¡ç®—é€šé“ç»´ä¸Šï¼ˆaxis=1ï¼‰çš„å‡å€¼å’Œæ–¹å·®ã€‚
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒXçš„å½¢çŠ¶ä»¥ä¾¿åé¢å¯ä»¥åšå¹¿æ’­è¿ç®—
            # æ²¿ç€ç¬¬0ã€2ã€3ç»´åº¦è®¡ç®—å‡å€¼
            # ä¸æ²¿ç€ dim=1ï¼ˆé€šé“ç»´åº¦ï¼‰ è®¡ç®—å‡å€¼ï¼Œè¿™æ„å‘³ç€ä¸ºæ¯ä¸ªé€šé“ç‹¬ç«‹è®¡ç®—å‡å€¼ã€‚
            # keepdim=True çš„ä½œç”¨
            # ä¿æŒè¾“å‡ºå¼ é‡çš„ç»´åº¦æ•°ç›®ä¸å˜ï¼Œåªæ˜¯å°†æ±‚å‡å€¼çš„ç»´åº¦å¤§å°å˜ä¸º1ã€‚
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œç”¨å½“å‰çš„å‡å€¼å’Œæ–¹å·®åšæ ‡å‡†åŒ–
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # æ›´æ–°ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®
        # é«˜momentumè®©æ¨¡å‹åƒç»éªŒä¸°å¯Œçš„è€ä¸“å®¶ï¼ŒåŸºäºé•¿æœŸç§¯ç´¯åšå‡ºåˆ¤æ–­ï¼Œ
        # ä¸ä¼šå› ä¸ºä¸€æ¬¡ç‰¹æ®Šæƒ…å†µå°±æ”¹å˜æ ¸å¿ƒè®¤çŸ¥ï¼
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # ç¼©æ”¾å’Œç§»ä½
    return Y, moving_mean.data, moving_var.data

class BatchNorm(nn.Module):
    # num_featuresï¼šå®Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æˆ–å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚
    # num_dimsï¼š2è¡¨ç¤ºå®Œå…¨è¿æ¥å±‚ï¼Œ4è¡¨ç¤ºå·ç§¯å±‚
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # å‚ä¸æ±‚æ¢¯åº¦å’Œè¿­ä»£çš„æ‹‰ä¼¸å’Œåç§»å‚æ•°ï¼Œåˆ†åˆ«åˆå§‹åŒ–æˆ1å’Œ0
        # Î³å’ŒÎ²æ˜¯éœ€è¦ä¸å…¶ä»–æ¨¡å‹å‚æ•°ä¸€èµ·å­¦ä¹ çš„å‚æ•°
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # éæ¨¡å‹å‚æ•°çš„å˜é‡åˆå§‹åŒ–ä¸º0å’Œ1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # å¦‚æœXä¸åœ¨å†…å­˜ä¸Šï¼Œå°†moving_meanå’Œmoving_var
        # å¤åˆ¶åˆ°Xæ‰€åœ¨æ˜¾å­˜ä¸Š
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
        self.moving_var = self.moving_var.to(X.device)
        # ä¿å­˜æ›´æ–°è¿‡çš„moving_meanå’Œmoving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y

def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦æ‰§è¡Œçš„ä»£ç """
    print("ğŸš€ Section05.py çš„ä¸»å‡½æ•°")
    # LeNetæ¨¡å‹-ä½¿ç”¨è‡ªå®šä¹‰çš„æ‰¹é‡è§„èŒƒåŒ–å±‚
    # net = nn.Sequential(
    #     nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),
    #     nn.AvgPool2d(kernel_size=2, stride=2),
    #     nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),
    #     nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    #     nn.Linear(16 * 4 * 4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),
    #     nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),
    #     nn.Linear(84, 10))
    # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å®šä¹‰çš„BatchNorm
    # é€šå¸¸é«˜çº§APIå˜ä½“è¿è¡Œé€Ÿåº¦å¿«å¾—å¤šï¼Œå› ä¸ºå®ƒçš„ä»£ç å·²ç¼–è¯‘
    # ä¸ºC++æˆ–CUDAï¼Œè€Œæˆ‘ä»¬çš„è‡ªå®šä¹‰ä»£ç ç”±Pythonå®ç°ã€‚
    net = nn.Sequential(
        nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
        nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),
        nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),
        nn.Linear(84, 10))
    lr, num_epochs, batch_size = 0.5, 10, 256
    train_iter, test_iter = Section01.load_data_fashion_mnist(batch_size)
    Section01.train_ch7(net, train_iter, test_iter, num_epochs, lr, Section01.try_gpu())

# åªæœ‰ç›´æ¥è¿è¡Œæœ¬æ–‡ä»¶æ—¶æ‰æ‰§è¡Œmain()
if __name__ == '__main__':
    main()
